<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Methods to Prevent Overfitting in Deep Learning | ChrisChen - 尾張</title>
<meta name=keywords content><meta name=description content="Methods to Prevent Overfitting in Deep Learning
Overfitting
Overfitting refers to that when a model fits the training data well but cannot predict the test data correctly, we may say that the model lacks the ability of generalization. It is important to figure out how it happens, and how we can prevent overfitting from the very beginning.
Detect Overfitting
The simplest way to detect overfitting is to split the dataset into two parts: the training set for training the model, and the test set for testing the accuracy of the model on a dataset that it has never seen before. Of course, we will also partition part of the training set to be the validation set for fine-tuning hyper-parameters. Note that it is necessary to shuffle all the data before splitting."><meta name=author content><link rel=canonical href=https://prov1dence.top/posts/deep-learning/methods-to-prevent-overfitting-in-deep-learning/><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://prov1dence.top/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://prov1dence.top/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://prov1dence.top/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://prov1dence.top/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://prov1dence.top/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://prov1dence.top/posts/deep-learning/methods-to-prevent-overfitting-in-deep-learning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://prov1dence.top/posts/deep-learning/methods-to-prevent-overfitting-in-deep-learning/"><meta property="og:site_name" content="ChrisChen - 尾張"><meta property="og:title" content="Methods to Prevent Overfitting in Deep Learning"><meta property="og:description" content="Methods to Prevent Overfitting in Deep Learning Overfitting Overfitting refers to that when a model fits the training data well but cannot predict the test data correctly, we may say that the model lacks the ability of generalization. It is important to figure out how it happens, and how we can prevent overfitting from the very beginning.
Detect Overfitting The simplest way to detect overfitting is to split the dataset into two parts: the training set for training the model, and the test set for testing the accuracy of the model on a dataset that it has never seen before. Of course, we will also partition part of the training set to be the validation set for fine-tuning hyper-parameters. Note that it is necessary to shuffle all the data before splitting."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-03-20T09:55:04+11:00"><meta property="article:modified_time" content="2019-03-20T09:55:04+11:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Methods to Prevent Overfitting in Deep Learning"><meta name=twitter:description content="Methods to Prevent Overfitting in Deep Learning
Overfitting
Overfitting refers to that when a model fits the training data well but cannot predict the test data correctly, we may say that the model lacks the ability of generalization. It is important to figure out how it happens, and how we can prevent overfitting from the very beginning.
Detect Overfitting
The simplest way to detect overfitting is to split the dataset into two parts: the training set for training the model, and the test set for testing the accuracy of the model on a dataset that it has never seen before. Of course, we will also partition part of the training set to be the validation set for fine-tuning hyper-parameters. Note that it is necessary to shuffle all the data before splitting."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://prov1dence.top/posts/"},{"@type":"ListItem","position":2,"name":"Methods to Prevent Overfitting in Deep Learning","item":"https://prov1dence.top/posts/deep-learning/methods-to-prevent-overfitting-in-deep-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Methods to Prevent Overfitting in Deep Learning","name":"Methods to Prevent Overfitting in Deep Learning","description":"Methods to Prevent Overfitting in Deep Learning Overfitting Overfitting refers to that when a model fits the training data well but cannot predict the test data correctly, we may say that the model lacks the ability of generalization. It is important to figure out how it happens, and how we can prevent overfitting from the very beginning.\nDetect Overfitting The simplest way to detect overfitting is to split the dataset into two parts: the training set for training the model, and the test set for testing the accuracy of the model on a dataset that it has never seen before. Of course, we will also partition part of the training set to be the validation set for fine-tuning hyper-parameters. Note that it is necessary to shuffle all the data before splitting.\n","keywords":[],"articleBody":"Methods to Prevent Overfitting in Deep Learning Overfitting Overfitting refers to that when a model fits the training data well but cannot predict the test data correctly, we may say that the model lacks the ability of generalization. It is important to figure out how it happens, and how we can prevent overfitting from the very beginning.\nDetect Overfitting The simplest way to detect overfitting is to split the dataset into two parts: the training set for training the model, and the test set for testing the accuracy of the model on a dataset that it has never seen before. Of course, we will also partition part of the training set to be the validation set for fine-tuning hyper-parameters. Note that it is necessary to shuffle all the data before splitting.\nThrough this split, we check the performance of the model to gain insight on how the training process goes, and detect overfitting.\nDuring training, we may see the process:\nThe accuracy on the training set has reached a very high ratio, but the accuracy on the validation set still remains not high enough, or even a little bit low.\nIf we plot the accuracy and loss of the training set and the validation set into one figure, we will see:\nIt is obvious that the accuracy of the validation data is much lower than that of the training data.\nAlthough overfitting is frustrating, there are plenty of methods to prevent it from happening.\nPrevent Overfitting Here are some practical methods to prevent overfitting during training deep neural networks:\n1. Regularization Regularization is the most-used method to prevent overfitting in Machine Learning. It constrains the learning of the model by adding a regularization term. Typical regularization is to explicitly add regularization terms in the objective function, e.g. L1 and L2 regularization terms.\nIn deep learning, there are two commonly-used regularization methods: Batch Normalization and Dropout.\nBatch Normalization As the training progresses, the parameters in the deep neural network are constantly updated. On the one hand, parameters are changing slightly during the training. Due to the activation functions in each layer, these slight changes are amplified as the number of layers deepens, and the input distribution of each layer changes; on the other hand, with the change of parameters, former layers need to adapt to these distribution changes, which makes it more difficult to train the model. These are called Internal Covariate Shift.\nBatch Normalization is proposed to solve these two problems. We perform normalization on each layer separately, making the features of each layer have a mean of 0 and a variance of 1, which lets the value of each layer propagate in the effective range. we can also add a linear transformation operation, so that the data can restore the ability of expression.\nThe intuition of Batch Normalization is not to prevent over-fitting or prevent the gradient from vanishing or exploding but to increase the robustness by normalizing the parameter. This constraint also improves the structural rationality of the system, which brings a series of improvements, e.g. accelerate convergence, prevent over-fitting, etc.\nDropout When we are training the model, we can set a probability P for eliminating a node in the deep neural network. For each node, there is a probability of (1-P) for keeping it and a probability of P for dropping it. Then we perform forward-propagation and backpropagation-propagation on this much-diminished deep neural network.\nAt each training step of a mini-batch dataset, the process of dropout creates a different deep neural network by randomly removing some units regarding the probability P. The process of dropout is similar to use ensemble learning on many different deep neural networks, each trained with a separate mini-batch dataset but share some context in the process of training.\nIn ensemble learning, since each classifier has been trained separately, it has learned different aspects of the dataset and their mistakes are different. Combining them helps to produce a more accurate classifier, which is less prone to overfitting. We can view dropout as a form of ensemble learning, and this is why it can prevent overfitting.\n2. Data Augmentation In Deep Learning, collecting data is an effective way to enhance the training but also a tedious and intricate process. In fact, for the majority of image recognition problems, we cannot get as much data as we expect. In order to obtain more data, data augmentation techniques is a very efficient method to improve the result.\nFor Convolutional Neural Networks, some common image augmentation techniques are listed:\nmirroring/flipping (on vertical or horizontal axis) rotating cropping scaling warping color shifting adding noise There are two ways to do data augmentation: Offline Augmentation and Online Augmentation.\nOffline Augmentation Offline Augmentation is to perform all augmentations in advance, before training, and save all the augmented data in memory. In fact, it will increase, or multiple, the size of the dataset. This method fits small datasets well.\nOnline Augmentation Another option, Online Augmentation, is to perform these augmentations on a mini-batch dataset right before they are fed to the model. This method is more suitable for large datasets because we do not have enough memory for staging all the augmented data when its scale is too enormous, so what we do is to perform mini-batch augmentation before feeding the current batch of data into the model.\n3. Early Stopping Early Stopping is a trade-off between training epochs and validation accuracy. At the end of each epoch, compare current validation accuracy of this epoch with the best validation accuracy. If the accuracy on the validation set decreases or does not reach the best one for more than 10 consecutive epochs, we stop the training, and we think the accuracy is no longer improved.\n4. Simplify The Model If we have done all the methods above to prevent overfitting but the performance is still bad, we may consider simplifying our model. The model may be too complicated for our dataset to fit and we can try to reduce the complexity of the model in some ways, e.g. reduce the number of layers, remove some neurons, etc.\n","wordCount":"1015","inLanguage":"en","datePublished":"2019-03-20T09:55:04+11:00","dateModified":"2019-03-20T09:55:04+11:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://prov1dence.top/posts/deep-learning/methods-to-prevent-overfitting-in-deep-learning/"},"publisher":{"@type":"Organization","name":"ChrisChen - 尾張","logo":{"@type":"ImageObject","url":"https://prov1dence.top/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://prov1dence.top/ accesskey=h title="尾張 (Alt + H)">尾張</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://prov1dence.top/archives/ title=Posts><span>Posts</span></a></li><li><a href=https://prov1dence.top/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Methods to Prevent Overfitting in Deep Learning</h1><div class=post-meta><span title='2019-03-20 09:55:04 +1100 AEDT'>March 20, 2019</span>&nbsp;·&nbsp;5 min</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#methods-to-prevent-overfitting-in-deep-learning aria-label="Methods to Prevent Overfitting in Deep Learning">Methods to Prevent Overfitting in Deep Learning</a><ul><li><a href=#overfitting aria-label=Overfitting>Overfitting</a></li><li><a href=#detect-overfitting aria-label="Detect Overfitting">Detect Overfitting</a></li><li><a href=#prevent-overfitting aria-label="Prevent Overfitting">Prevent Overfitting</a><ul><li><a href=#1-regularization aria-label="1. Regularization">1. Regularization</a><ul><li><a href=#batch-normalization aria-label="Batch Normalization">Batch Normalization</a></li><li><a href=#dropout aria-label=Dropout>Dropout</a></li></ul></li><li><a href=#2-data-augmentation aria-label="2. Data Augmentation">2. Data Augmentation</a><ul><li><a href=#offline-augmentation aria-label="Offline Augmentation">Offline Augmentation</a></li><li><a href=#online-augmentation aria-label="Online Augmentation">Online Augmentation</a></li></ul></li><li><a href=#3-early-stopping aria-label="3. Early Stopping">3. Early Stopping</a></li><li><a href=#4-simplify-the-model aria-label="4. Simplify The Model">4. Simplify The Model</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h1 id=methods-to-prevent-overfitting-in-deep-learning>Methods to Prevent Overfitting in Deep Learning<a hidden class=anchor aria-hidden=true href=#methods-to-prevent-overfitting-in-deep-learning>#</a></h1><h2 id=overfitting>Overfitting<a hidden class=anchor aria-hidden=true href=#overfitting>#</a></h2><p>Overfitting refers to that when a model fits the training data well but cannot predict the test data correctly, we may say that the model lacks the ability of generalization. It is important to figure out how it happens, and how we can prevent overfitting from the very beginning.</p><h2 id=detect-overfitting>Detect Overfitting<a hidden class=anchor aria-hidden=true href=#detect-overfitting>#</a></h2><p>The simplest way to detect overfitting is to split the dataset into two parts: the training set for training the model, and the test set for testing the accuracy of the model on a dataset that it has never seen before. Of course, we will also partition part of the training set to be the validation set for fine-tuning hyper-parameters. Note that it is necessary to shuffle all the data before splitting.</p><p>Through this split, we check the performance of the model to gain insight on how the training process goes, and detect overfitting.</p><p>During training, we may see the process:</p><img src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/Deep-Learning/0.png><p>The accuracy on the training set has reached a very high ratio, but the accuracy on the validation set still remains not high enough, or even a little bit low.</p><p>If we plot the accuracy and loss of the training set and the validation set into one figure, we will see:</p><img src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/Deep-Learning/1.png><p>It is obvious that the accuracy of the validation data is much lower than that of the training data.</p><p>Although overfitting is frustrating, there are plenty of methods to prevent it from happening.</p><h2 id=prevent-overfitting>Prevent Overfitting<a hidden class=anchor aria-hidden=true href=#prevent-overfitting>#</a></h2><p>Here are some practical methods to prevent overfitting during training deep neural networks:</p><h3 id=1-regularization>1. Regularization<a hidden class=anchor aria-hidden=true href=#1-regularization>#</a></h3><p>Regularization is the most-used method to prevent overfitting in Machine Learning. It constrains the learning of the model by adding a regularization term. Typical regularization is to explicitly add regularization terms in the objective function, e.g. L1 and L2 regularization terms.</p><p>In deep learning, there are two commonly-used regularization methods: <strong>Batch Normalization</strong> and <strong>Dropout</strong>.</p><h4 id=batch-normalization>Batch Normalization<a hidden class=anchor aria-hidden=true href=#batch-normalization>#</a></h4><p>As the training progresses, the parameters in the deep neural network are constantly updated. On the one hand, parameters are changing slightly during the training. Due to the activation functions in each layer, these slight changes are amplified as the number of layers deepens, and the input distribution of each layer changes; on the other hand, with the change of parameters, former layers need to adapt to these distribution changes, which makes it more difficult to train the model. These are called Internal Covariate Shift.</p><p>Batch Normalization is proposed to solve these two problems. We perform normalization on each layer separately, making the features of each layer have a mean of 0 and a variance of 1, which lets the value of each layer propagate in the effective range. we can also add a linear transformation operation, so that the data can restore the ability of expression.</p><p>The intuition of Batch Normalization is not to prevent over-fitting or prevent the gradient from vanishing or exploding but to increase the robustness by normalizing the parameter. This constraint also improves the structural rationality of the system, which brings a series of improvements, e.g. accelerate convergence, prevent over-fitting, etc.</p><h4 id=dropout>Dropout<a hidden class=anchor aria-hidden=true href=#dropout>#</a></h4><p>When we are training the model, we can set a probability P for eliminating a node in the deep neural network. For each node, there is a probability of (1-P) for keeping it and a probability of P for dropping it. Then we perform forward-propagation and backpropagation-propagation on this much-diminished deep neural network.</p><p>At each training step of a mini-batch dataset, the process of dropout creates a different deep neural network by randomly removing some units regarding the probability P. The process of dropout is similar to use ensemble learning on many different deep neural networks, each trained with a separate mini-batch dataset but share some context in the process of training.</p><p>In ensemble learning, since each classifier has been trained separately, it has learned different aspects of the dataset and their mistakes are different. Combining them helps to produce a more accurate classifier, which is less prone to overfitting. We can view dropout as a form of ensemble learning, and this is why it can prevent overfitting.</p><h3 id=2-data-augmentation>2. Data Augmentation<a hidden class=anchor aria-hidden=true href=#2-data-augmentation>#</a></h3><p>In Deep Learning, collecting data is an effective way to enhance the training but also a tedious and intricate process. In fact, for the majority of image recognition problems, we cannot get as much data as we expect. In order to obtain more data, <strong>data augmentation</strong> techniques is a very efficient method to improve the result.</p><p>For Convolutional Neural Networks, some common image augmentation techniques are listed:</p><ul><li>mirroring/flipping (on vertical or horizontal axis)</li><li>rotating</li><li>cropping</li><li>scaling</li><li>warping</li><li>color shifting</li><li>adding noise</li></ul><p>There are two ways to do data augmentation: <strong>Offline Augmentation</strong> and <strong>Online Augmentation</strong>.</p><h4 id=offline-augmentation>Offline Augmentation<a hidden class=anchor aria-hidden=true href=#offline-augmentation>#</a></h4><p><strong>Offline Augmentation</strong> is to perform all augmentations in advance, before training, and save all the augmented data in memory. In fact, it will increase, or multiple, the size of the dataset. This method fits small datasets well.</p><h4 id=online-augmentation>Online Augmentation<a hidden class=anchor aria-hidden=true href=#online-augmentation>#</a></h4><p>Another option, <strong>Online Augmentation</strong>, is to perform these augmentations on a mini-batch dataset right before they are fed to the model. This method is more suitable for large datasets because we do not have enough memory for staging all the augmented data when its scale is too enormous, so what we do is to perform mini-batch augmentation before feeding the current batch of data into the model.</p><h3 id=3-early-stopping>3. Early Stopping<a hidden class=anchor aria-hidden=true href=#3-early-stopping>#</a></h3><p>Early Stopping is a trade-off between training epochs and validation accuracy. At the end of each epoch, compare current validation accuracy of this epoch with the best validation accuracy. If the accuracy on the validation set decreases or does not reach the best one for more than 10 consecutive epochs, we stop the training, and we think the accuracy is no longer improved.</p><h3 id=4-simplify-the-model>4. Simplify The Model<a hidden class=anchor aria-hidden=true href=#4-simplify-the-model>#</a></h3><p>If we have done all the methods above to prevent overfitting but the performance is still bad, we may consider simplifying our model. The model may be too complicated for our dataset to fit and we can try to reduce the complexity of the model in some ways, e.g. reduce the number of layers, remove some neurons, etc.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://prov1dence.top/posts/kick-start/2019-round-a/><span class=title>« Prev</span><br><span>Kick Start 2019 Round A</span>
</a><a class=next href=https://prov1dence.top/posts/cpp/smart-pointer/c++-smart-pointer-3/><span class=title>Next »</span><br><span>C++ 智能指针（3）：shared_ptr</span></a></nav></footer></article></main><footer class=footer></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>