<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>启发式搜索和强化学习 | ChrisChen - 尾張</title>
<meta name=keywords content><meta name=description content="启发式搜索和强化学习
The Pac-Man Projects 是 UC Berkeley CS 188 的课程项目，本文以该项目为例介绍启发式搜索和强化学习。
1 盲目搜索
盲目搜索（Blind Search）指不利用任何额外信息（输入数据，或辅助函数），只依赖于算法本身的搜索，例如 BFS，DFS，Dijkstra 等；
DFS
The Pac-Man Projects  已经实现了吃豆人游戏的后台逻辑和图形渲染框架，我们只需要在 search.py 文件中实现具体的搜索算法，并根据搜索算法生成寻路路径，即可让吃豆人移动，先来实现一个简单的 DFS：
def DepthFirstSearch(problem):
    from util import Stack
    open_list = Stack()
    visited = []
    open_list.push((problem.getStartState(), []))
    while not open_list.isEmpty():
        current_node, path = open_list.pop()
        if problem.isGoalState(current_node):
            return path
        if current_node in visited:
            continue
        visited.append(current_node)
        for next_node, action, cost in problem.getSuccessors(current_node):
            if next_node not in visited:
                open_list.push((next_node, path + [action]))
dfs = DepthFirstSearch
在吃豆人游戏的框架下，为寻路函数传入的 problem 参数可以理解为一个 class SearchProblem 类型的抽象基类，实际的问题有 PositionSearchProblem（找到单个终点），FoodSearchProblem（找到所有食物），CapsuleSearchProblem（找到增益药丸和所有食物）等，这些子类都需要实现以下函数："><meta name=author content><link rel=canonical href=https://photography.prov1dence.top/posts/reinforcement-learning/heuristic-search-and-reinforcement-learning/><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://photography.prov1dence.top/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://photography.prov1dence.top/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://photography.prov1dence.top/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://photography.prov1dence.top/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://photography.prov1dence.top/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://photography.prov1dence.top/posts/reinforcement-learning/heuristic-search-and-reinforcement-learning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://photography.prov1dence.top/posts/reinforcement-learning/heuristic-search-and-reinforcement-learning/"><meta property="og:site_name" content="ChrisChen - 尾張"><meta property="og:title" content="启发式搜索和强化学习"><meta property="og:description" content="启发式搜索和强化学习 The Pac-Man Projects 是 UC Berkeley CS 188 的课程项目，本文以该项目为例介绍启发式搜索和强化学习。
1 盲目搜索 盲目搜索（Blind Search）指不利用任何额外信息（输入数据，或辅助函数），只依赖于算法本身的搜索，例如 BFS，DFS，Dijkstra 等；
DFS The Pac-Man Projects 已经实现了吃豆人游戏的后台逻辑和图形渲染框架，我们只需要在 search.py 文件中实现具体的搜索算法，并根据搜索算法生成寻路路径，即可让吃豆人移动，先来实现一个简单的 DFS：
def DepthFirstSearch(problem): from util import Stack open_list = Stack() visited = [] open_list.push((problem.getStartState(), [])) while not open_list.isEmpty(): current_node, path = open_list.pop() if problem.isGoalState(current_node): return path if current_node in visited: continue visited.append(current_node) for next_node, action, cost in problem.getSuccessors(current_node): if next_node not in visited: open_list.push((next_node, path + [action])) dfs = DepthFirstSearch 在吃豆人游戏的框架下，为寻路函数传入的 problem 参数可以理解为一个 class SearchProblem 类型的抽象基类，实际的问题有 PositionSearchProblem（找到单个终点），FoodSearchProblem（找到所有食物），CapsuleSearchProblem（找到增益药丸和所有食物）等，这些子类都需要实现以下函数："><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-12-10T20:07:52+08:00"><meta property="article:modified_time" content="2020-12-10T20:07:52+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="启发式搜索和强化学习"><meta name=twitter:description content="启发式搜索和强化学习
The Pac-Man Projects 是 UC Berkeley CS 188 的课程项目，本文以该项目为例介绍启发式搜索和强化学习。
1 盲目搜索
盲目搜索（Blind Search）指不利用任何额外信息（输入数据，或辅助函数），只依赖于算法本身的搜索，例如 BFS，DFS，Dijkstra 等；
DFS
The Pac-Man Projects  已经实现了吃豆人游戏的后台逻辑和图形渲染框架，我们只需要在 search.py 文件中实现具体的搜索算法，并根据搜索算法生成寻路路径，即可让吃豆人移动，先来实现一个简单的 DFS：
def DepthFirstSearch(problem):
    from util import Stack
    open_list = Stack()
    visited = []
    open_list.push((problem.getStartState(), []))
    while not open_list.isEmpty():
        current_node, path = open_list.pop()
        if problem.isGoalState(current_node):
            return path
        if current_node in visited:
            continue
        visited.append(current_node)
        for next_node, action, cost in problem.getSuccessors(current_node):
            if next_node not in visited:
                open_list.push((next_node, path + [action]))
dfs = DepthFirstSearch
在吃豆人游戏的框架下，为寻路函数传入的 problem 参数可以理解为一个 class SearchProblem 类型的抽象基类，实际的问题有 PositionSearchProblem（找到单个终点），FoodSearchProblem（找到所有食物），CapsuleSearchProblem（找到增益药丸和所有食物）等，这些子类都需要实现以下函数："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://photography.prov1dence.top/posts/"},{"@type":"ListItem","position":2,"name":"启发式搜索和强化学习","item":"https://photography.prov1dence.top/posts/reinforcement-learning/heuristic-search-and-reinforcement-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"启发式搜索和强化学习","name":"启发式搜索和强化学习","description":"启发式搜索和强化学习 The Pac-Man Projects 是 UC Berkeley CS 188 的课程项目，本文以该项目为例介绍启发式搜索和强化学习。\n1 盲目搜索 盲目搜索（Blind Search）指不利用任何额外信息（输入数据，或辅助函数），只依赖于算法本身的搜索，例如 BFS，DFS，Dijkstra 等；\nDFS The Pac-Man Projects 已经实现了吃豆人游戏的后台逻辑和图形渲染框架，我们只需要在 search.py 文件中实现具体的搜索算法，并根据搜索算法生成寻路路径，即可让吃豆人移动，先来实现一个简单的 DFS：\ndef DepthFirstSearch(problem): from util import Stack open_list = Stack() visited = [] open_list.push((problem.getStartState(), [])) while not open_list.isEmpty(): current_node, path = open_list.pop() if problem.isGoalState(current_node): return path if current_node in visited: continue visited.append(current_node) for next_node, action, cost in problem.getSuccessors(current_node): if next_node not in visited: open_list.push((next_node, path + [action])) dfs = DepthFirstSearch 在吃豆人游戏的框架下，为寻路函数传入的 problem 参数可以理解为一个 class SearchProblem 类型的抽象基类，实际的问题有 PositionSearchProblem（找到单个终点），FoodSearchProblem（找到所有食物），CapsuleSearchProblem（找到增益药丸和所有食物）等，这些子类都需要实现以下函数：\n","keywords":[],"articleBody":"启发式搜索和强化学习 The Pac-Man Projects 是 UC Berkeley CS 188 的课程项目，本文以该项目为例介绍启发式搜索和强化学习。\n1 盲目搜索 盲目搜索（Blind Search）指不利用任何额外信息（输入数据，或辅助函数），只依赖于算法本身的搜索，例如 BFS，DFS，Dijkstra 等；\nDFS The Pac-Man Projects 已经实现了吃豆人游戏的后台逻辑和图形渲染框架，我们只需要在 search.py 文件中实现具体的搜索算法，并根据搜索算法生成寻路路径，即可让吃豆人移动，先来实现一个简单的 DFS：\ndef DepthFirstSearch(problem): from util import Stack open_list = Stack() visited = [] open_list.push((problem.getStartState(), [])) while not open_list.isEmpty(): current_node, path = open_list.pop() if problem.isGoalState(current_node): return path if current_node in visited: continue visited.append(current_node) for next_node, action, cost in problem.getSuccessors(current_node): if next_node not in visited: open_list.push((next_node, path + [action])) dfs = DepthFirstSearch 在吃豆人游戏的框架下，为寻路函数传入的 problem 参数可以理解为一个 class SearchProblem 类型的抽象基类，实际的问题有 PositionSearchProblem（找到单个终点），FoodSearchProblem（找到所有食物），CapsuleSearchProblem（找到增益药丸和所有食物）等，这些子类都需要实现以下函数：\ngetStartState()：获取起始状态； isGoalState(state)：判断 state 节点是否是目标节点； getSuccessors(statu)：获取 state 节点的所有后续节点； getCostOfActions(actions)：actions 是一个由上下左右方向组成的一个动作列表，函数返回这个列表的总花费（cost）； 运行一下看看 DFS 的效果：\n$ python pacman.py -l smallEmpty -z 0.8 -p SearchAgent -a fn=dfs [SearchAgent] using function dfs [SearchAgent] using problem type PositionSearchProblem Path found with total cost of 56 in 0.002992 seconds Search nodes expanded: 56 Pacman emerges victorious! Score: 454 Average Score: 454.0 Scores: 454.0 Win Rate: 1/1 (1.00) Record: Win 运行的参数列表中有几个参数：\n-l smallEmpty：在名为 smallEmpty 的地图上运行，地图定义在 layouts 目录下； -z 0.8：客户端表现缩放为 0.8 倍 -p SearchAgent：指定实际的问题，这里的 SearchAgent 是 fn='depthFirstSearch', prob='PositionSearchProblem' 的缩写； 实际运行效果如下：\n可以看到吃豆人 agent 绕了很远的路才到达终点，因为 DFS 在计算复杂性理论中是不完备（complete）且非最优（optimality）的。\nBFS def BreadthFirstSearch(problem): from util import Queue open_list = Queue() visited = set() open_list.push((problem.getStartState(), [])) while not open_list.isEmpty(): current_node, path = open_list.pop() if problem.isGoalState(current_node): return path if current_node in visited: continue visited.add(current_node) for next_node, action, cost in problem.getSuccessors(current_node): if next_node not in visited: open_list.push((next_node, path + [action])) bfs = BreadthFirstSearch BFS 的运行效果如下：\n$ python pacman.py -l smallEmpty -z 0.8 -p SearchAgent -a fn=bfs [SearchAgent] using function bfs [SearchAgent] using problem type PositionSearchProblem Path found with total cost of 14 in 0.001995 seconds Search nodes expanded: 63 Pacman emerges victorious! Score: 496 Average Score: 496.0 Scores: 496.0 Win Rate: 1/1 (1.00) Record: Win 可以看到使用 BFS 的 agent 通过最短路径到达了终点，因为 BFS 是完备且最优的。\nIterative Deepening Search IDS 的思路是重复进行限制层数的 DFS 来找到最优解，它综合了 DFS 的优点（空间复杂度）和 BFS 的优点（完备且最优），但是在时间复杂度上表现比较差（可以参考输出结果中的 Search nodes expanded）：\ndef IterativeDeepeningSearch(problem): import sys from util import Stack def depthLimitSearch(problem, depth): visited = [] open_list = Stack() open_list.push((problem.getStartState(), [], visited)) while not open_list.isEmpty(): current_node, path, visited = open_list.pop() if problem.isGoalState(current_node): return path if len(path) == depth or depth == 0: continue if current_node in visited: continue actions = problem.getSuccessors(current_node) for next_node, action, cost in actions: if next_node not in visited: open_list.push((next_node, path + [action], visited+[current_node])) for depth in range(sys.maxsize**10): path = depthLimitSearch(problem, depth) if path: return path ids = IterativeDeepeningSearch 这个算法对于只有小面积可搜索空间的地图效果比较好：\n$ python pacman.py -l smallMaze -z 0.8 -p SearchAgent -a fn=ids [SearchAgent] using function ids [SearchAgent] using problem type PositionSearchProblem Path found with total cost of 19 in 0.008976 seconds Search nodes expanded: 923 Pacman emerges victorious! Score: 491 Average Score: 491.0 Scores: 491.0 Win Rate: 1/1 (1.00) Record: Win 但是对于拥有大面积可搜索空间的地图，搜索时间会非常长：\n$ python pacman.py -l smallEmpty -z 0.8 -p SearchAgent -a fn=ids [SearchAgent] using function ids [SearchAgent] using problem type PositionSearchProblem Path found with total cost of 14 in 0.710854 seconds Search nodes expanded: 94552 Pacman emerges victorious! Score: 496 Average Score: 496.0 Scores: 496.0 Win Rate: 1/1 (1.00) Record: Win Uniform Cost Search UCS 和 Dijkstra 类似，用一个小根堆保存当前节点到起始节点的距离，依次展开路径花费最小的节点，直到找到终点为止，而一般来说 Dijkstra 没有一个固定的终点：\ndef UniformCostSearch(problem): from util import PriorityQueue frontier = PriorityQueue() visited = [] frontier.push((problem.getStartState(), [], 0), 0) while not frontier.isEmpty(): current_node, path, current_cost = frontier.pop() if problem.isGoalState(current_node): return path if current_node in visited: continue visited.append(current_node) for next_node, action, cost in problem.getSuccessors(current_node): if next_node not in visited: frontier.push((next_node, path + [action], current_cost + cost), current_cost + cost) ucs = UniformCostSearch $ python pacman.py -l smallEmpty -z 0.8 -p SearchAgent -a fn=ucs [SearchAgent] using function ucs [SearchAgent] using problem type PositionSearchProblem Path found with total cost of 14 in 0.002992 seconds Search nodes expanded: 63 Pacman emerges victorious! Score: 496 Average Score: 496.0 Scores: 496.0 Win Rate: 1/1 (1.00) Record: Win 2 启发式搜索 传统的盲目搜索算法因为受制于完备性、最优性、时间、空间复杂度等因素，在实际的应用中很少被使用；而在路径规划，最优化算法和人工智能领域，使用启发式搜索（Heuristic Search）能够更好地在准确性和计算速度之间取得平衡。\n启发式搜索 Heuristic Search 又叫做有信息搜索 Informed Search，启发式搜索不同于盲目搜索的地方有两点：一是启发式搜索依赖于启发函数，启发函数 Heuristic Function 是用于估计当前节点到目标节点距离的一类函数；二是它需要利用输入数据并将其作为启发函数的参数，以衡量当前位置到目标位置的距离关系。\n启发式搜索通过衡量当前位置到目标位置的距离关系，使得搜索过程的移动方向优先朝向目标位置更近的方向前进，以提高搜索效率。\n启发函数 启发函数 h(n) 用于给出从特定节点到目标节点的距离的估计值（而不是真实值）；许多寻路问题都是 NP 完备（NP-completeness）的，因此在最坏情况下它们的算法时间复杂度都是指数级的；找到一个好的启发函数可以更高效地得到一个更优的解；启发函数算法的优劣直接决定了启发式搜索的效率。\n最简单的启发函数有：\nnull heuristic：估计值始终等于 0，相当于退化成了 UCS（只计算当前节点到起始节点的距离）； 曼哈顿距离：两点在南北方向上的距离加上在东西方向上的距离，即 abs(a − x) + abs(b − y)； 欧几里得距离：两点在欧氏空间中的直线距离，即 sqrt((a - x) ^ 2 + (b - y) ^ 2)； A* A* 是一种应用很广泛的启发式搜索算法，其主要思路与 Dijkstra 和 UCS 类似，都是利用一个小根堆，不断地取出堆顶节点并判断其是否是目标节点，不同的是它会为每一个已知节点计算出从起点和终点的距离之和 f(x) = g(x) + h(x)，其中 g(x) 是从起点到当前节点的实际距离，h(x) 是使用启发函数计算得到的从当前节点到目标节点的估计距离：\ndef AStarSearch(problem, heuristic=nullHeuristic): from util import PriorityQueueWithFunction def AStarHeuristic(item): state, _, cost = item h = heuristic(state, problem=problem) g = cost return g + h frontier = PriorityQueueWithFunction(AStarHeuristic) visited = [] frontier.push((problem.getStartState(),[], 0)) while not frontier.isEmpty(): currentNode, path, currentCost = frontier.pop() if problem.isGoalState(currentNode): return path if currentNode not in visited: visited.append(currentNode) for nextNode, action, cost in problem.getSuccessors(currentNode): if nextNode not in visited: frontier.push((nextNode, path + [action], currentCost + cost)) astar = AStarSearch 对于多节点的搜索问题，需要综合考虑所有目标节点对于当前节点的影响；我们可以利用贪心的思想，让吃豆人优先靠近距离较近的豆子，也就是使得距离当前节点更近的目标节点的启发函数值更小，这样距离吃豆人更近的豆子就会更有可能具有更小的 f(x) 值；在为启发函数传入的参数中， state 是一个包含当前位置 position 和所有目标点信息结构 grid 的二元组，可以使用 grid.asList() 将所有目标点转换为一个数组：\ndef FoodHeuristic(state, problem): position, food_grid = state food_gridList = food_grid if isinstance(food_grid, list) else food_grid.asList() from util import manhattanDistance minx, miny = position maxx, maxy = position for food in food_gridList: foodx, foody = food minx = min(foodx,minx) maxx = max(foodx,maxx) miny = min(foody,miny) maxy = max(foody,maxy) return abs(minx-maxx) + abs(miny-maxy) $ python pacman.py -l tinySearch -p SearchAgent -a fn=astar,prob=FoodSearchProblem,heuristic=FoodHeuristic [SearchAgent] using function astar and heuristic foodHeuristic [SearchAgent] using problem type FoodSearchProblem Path found with total cost of 27 in 0.294214 seconds Search nodes expanded: 1544 Pacman emerges victorious! Score: 573 Average Score: 573.0 Scores: 573.0 Win Rate: 1/1 (1.00) Record: Win 换个地图看看效果：\n$ python pacman.py -l mediumDottedMaze -p SearchAgent -a fn=astar,prob=FoodSearchProblem,heuristic=FoodHeuristic [SearchAgent] using function astar and heuristic foodHeuristic [SearchAgent] using problem type FoodSearchProblem Path found with total cost of 74 in 0.091756 seconds Search nodes expanded: 389 Pacman emerges victorious! Score: 646 Average Score: 646.0 Scores: 646.0 Win Rate: 1/1 (1.00) Record: Win 相比之下，如果使用 nullHeuristic（退化为 UCS）的话搜索花费的时间则会长很多：\n$ python pacman.py -l tinySearch -p SearchAgent -a fn=ucs,prob=FoodSearchProblem [SearchAgent] using function ucs [SearchAgent] using problem type FoodSearchProblem Path found with total cost of 27 in 2.880744 seconds Search nodes expanded: 5057 Pacman emerges victorious! Score: 573 Average Score: 573.0 Scores: 573.0 Win Rate: 1/1 (1.00) Record: Win 3 强化学习 强化学习 强化学习是指通过与环境进行交互和反馈来学习一种策略的过程，在这个过程中，一个强化学习的实体 agent 通过与环境 Environment 进行交互并采取一系列行为 Action 来获得一定的收益 Reward，从而更新采取相应行为的权重。\n强化学习的目标是通过学习得到某个策略 Policy，使得 agent 从 environment 中获得的长期收益最大化，因此在一般的问题中，在没有达到最终的目的前，reward 通常都是负数（随时间的增加而减少），而仅在达到最终的目的时获得较大的正反馈，这样的学习任务通常称为 episodic task（例如吃豆人游戏中的单节点搜索问题）；在另一类问题中，可能需要完成多个目标才能到达最终状态，其 reward 离散地分布在一个连续的空间中，这一类任务称为 continuing task（例如吃豆人游戏中的多节点搜索问题），对于 continuing task，我们可以定义其 reward 为：\n其中 γ 是衰减率（discount factor），衰减率可以使得我们更加偏好近期收益；引入衰减系数的理由有很多，例如避免陷入无限循环，降低远期利益的不确定性，最大化近期利益，利用近期利益产生新的利益因而其更有价值等等。\n而强化学习的结果是就是 Gt，通过 argmax 取得的值能够给出在每个状态下我们应该采取的行动，我们可以把这个策略记做 π(a|s)，它表示在状态 s 下采取行动 a 的概率。\n马尔科夫决策过程 马尔科夫决策过程（Markov Decision Process, MDP）是指在每个状态下，agent 对于行动 a 的选取只依赖于当前的状态，与任何之前的行为都没有关系；几乎所有的强化学习问题都可以使用 MDP 解决，一个标准的马尔可夫决策过程由一个四元组组成：\nS：State，状态空间的集合，S0 表示初始状态； A：Action，行为空间的集合，包含每个状态可以进行的动作； r(s’ | s, a)：Reward，在 s 状态下，进行 a 操作并转移到 s‘ 状态下的奖励； P(s’ | s, a)：Probability，在 s 状态下，进行 a 操作并转移到 s‘ 状态下的概率； 求解 MDP 问题的常见方法有 Value iteration，Policy iteration，Q-Learning，Deep Q-Learning Network 等等。\nValue Iteration Value Iteration 是一种基于模型的（model-based）算法，使用 Value Iteration 来解决 MDP 问题的前提是我们知道关于模型的所有信息，即 MDP 四元组的所有内容。\n假设现在有一个 3*4 叫做 GridWorld 的地图如图所示，以左下角格子为 (0, 0) 原点，其中 (1, 1) 为不可通过的墙，(2, 3) 为奖励为 +1 的终点，(1, 3) 为 -1 的终点；我们定义每一个位置的价值为 V(state)，即对于 state(x, y)，V(state) 表示其能获取的最大价值；每一个位置初始化时其 value 均为 0：\n在迭代过程中，使用贝尔曼方程（Bellman Equation）更新所有位置的 value，它描述了最佳策略必须满足的条件，前半部分 r(s, a, s’) 代表采取了 a 行为之后得到的 reward，后半部分；我们需要在每轮迭代中计算每个状态的价值即 V(s)，直到两次迭代结果的差值小于给定的阈值才能认为其收敛了，这里的 V(s) 也叫做 q-value：\n经过前三次迭代分别得到：\n收敛速度是指数级，并且随着迭代的不断进行，终将得到最优的 V(s)；或者说当迭代次数趋近于无穷大的时候，将得到 V(s) 的最优解；经过 100 次迭代后将得到：\n取 argmax 即可得到最优的策略（即上图中的小箭头）；也可以看到采取每一种行动对应的 Probaility：\n进行 Value Iteration 的流程主要对应 runValueIteration 和 computeQValueFromValues 两个函数，迭代结束后选择策略则对应 computeActionFromValues 函数：\n# valueIterationAgents.py class ValueIterationAgent(ValueEstimationAgent): \"\"\" A ValueIterationAgent takes a Markov decision process (see mdp.py) on initialization and runs value iteration for a given number of iterations using the supplied discount factor. \"\"\" def __init__(self, mdp, discount = 0.9, iterations = 100): \"\"\" Some useful mdp methods you will use: mdp.getStates() mdp.getPossibleActions(state) mdp.getTransitionStatesAndProbs(state, action) mdp.getReward(state, action, nextState) mdp.isTerminal(state) \"\"\" self.mdp = mdp self.discount = discount self.iterations = iterations self.values = util.Counter() # A Counter is a dict with default 0 self.runValueIteration() def runValueIteration(self): for _ in np.arange(0, self.iterations): next_values = util.Counter() for state in self.mdp.getStates(): if self.mdp.isTerminal(state): continue q_values = util.Counter() for action in self.mdp.getPossibleActions(state): q_values[action] = self.computeQValueFromValues(state, action) key_max_value = q_values.argMax() next_values[state] = q_values[key_max_value] self.values = next_values def getValue(self, state): return self.values[state] def computeQValueFromValues(self, state, action): \"\"\" Compute the Q-value of action in state from the value function stored in self.values. \"\"\" next_states_probs = self.mdp.getTransitionStatesAndProbs(state, action) q_value = 0 for (next_state, next_state_prob) in next_states_probs: q_value += next_state_prob * (self.mdp.getReward(state, action, next_state) + self.discount * self.values[next_state]) return q_value def computeActionFromValues(self, state): \"\"\" The policy is the best action in the given state according to the values currently stored in self.values. You may break ties any way you see fit. Note that if there are no legal actions, which is the case at the terminal state, you should return None. \"\"\" if self.mdp.isTerminal(state): return None actions = self.mdp.getPossibleActions(state) values = util.Counter() for action in actions: values[action] = self.computeQValueFromValues(state, action) policy = values.argMax() return policy def getPolicy(self, state): return self.computeActionFromValues(state) def getAction(self, state): \"Returns the policy at the state (no exploration).\" return self.computeActionFromValues(state) def getQValue(self, state, action): return self.computeQValueFromValues(state, action) Q-Learning Q-Learning 的思路与 Value Iteration 有一些类似，但它是一种模型无关的（model-free）算法，使用 Q-Learning 的时候我们的 agent 无需事先知道当前环境中的 State，Action 等 MDP 四元组内容，\n在使用 Value Ietration 的时候，我们需要在每一个 episode 对所有的 State 和 Action 进行更新，但在实际问题中 State 的数量可能非常多以致于我们不可能遍历完所有的状态，这时候我们可以借助 Q-Learning，在对于环境未知的前提下，不断地与环境进行交互和探索，计算出有限的环境样本中 Q-Value，并维护一个 Q-Table：\nS r(s’ | s, action 1) r(s’ | s, action 2) … S1 (0, 0) 3 -1 S2 (0, 1) -2 4 … 在刚开始时，agent 对于环境一无所知，因此 Q-Table 应该被初始化为一个零矩阵；当我们处于某个状态 s （例如表里的 S1）时，根据 Q-Table 中当前的最优值和一定的策略（Multi-armed bandit 问题，利用 ε-greedy，UCB 等解决）选择对应的动作 a （假设选择了表里的 action 1，对应 r = 3）进行探索，并根据获得的即时奖励 r 来更新奖励，这里的 r 只是即时获得的奖励（r = 3），因为还要考虑所转移到的状态 s’ （表里的 S2）在未来可能会获取到的最大奖励（r’ = 4）；\n真正的奖励 Q(St, At) 由公式中的两部分组成，前半部分 r 是通过动作 a 即时获得的奖励（r = 3），后半部分 γ * max(a’)Q(s’, a’) 是对未来行为的最大期望奖励（r’ = 4），且后半部分往往是不确定的，因此需要乘以衰减率 γ：\n在通过计算得到当前行为所能获得的预期奖励后，将其减去表中对当前环境的估计奖励 Q(s,a)（r = 3），再乘以学习率，就能用来更新 Q-Table 中的值了。\nagent 不断地与环境进行探索并发生状态转换，直到到达目标；我们将 agent 的每一轮探索（从任意起始状态出发，经历若干个 action，直到到达目标状态）称为一个 episode；在进行指定 episode 次数的训练之后，取 argmax 得到的策略就是当前的最优解。\n现在利用 epsilon-greedy 作为探索策略，训练 10 个 episode：\n$ python gridworld.py -a q -k 10 --noise 0.0 -e 0.9 得到结果之后，对每一个状态 s 的所有动作 a 取 argmax 即可得到在当前 epsilon 值和 episode 值下的最优解：\nQ-Learning 的实现大致如下：\nclass QLearningAgent(ReinforcementAgent): def __init__(self, **args): ReinforcementAgent.__init__(self, **args) self.q_values = defaultdict(lambda: 0.0) def getQValue(self, state, action): \"\"\" Returns Q(state,action) Should return 0.0 if we have never seen a state or the Q node value otherwise \"\"\" return self.q_values[(state, action)] def computeValueFromQValues(self, state): \"\"\" Returns max_action Q(state,action) where the max is over legal actions. Note that if there are no legal actions, which is the case at the terminal state, you should return a value of 0.0. \"\"\" next_actions = self.getLegalActions(state) if not next_actions: return 0.0 else: q_value_actions = [(self.getQValue(state, action), action) for action in next_actions] # return the max in q_value_actions which is q_value return sorted(q_value_actions, key=lambda x: x[0])[-1][0] def computeActionFromQValues(self, state): \"\"\" Compute the best action to take in a state. Note that if there are no legal actions, which is the case at the terminal state, you should return None. \"\"\" next_actions = self.getLegalActions(state) if not next_actions: return None else: actions = [] max_q_value = self.getQValue(state, next_actions[0]) # find actions with max q value for action in next_actions: action_q_value = self.getQValue(state, action) if max_q_value \u003c action_q_value: max_q_value = action_q_value actions = [action] elif max_q_value == action_q_value: actions.append(action) # break ties randomly for better behavior. The random.choice() function will help. return random.choice(actions) def getAction(self, state): \"\"\" Compute the action to take in the current state. With probability self.epsilon, we should take a random action and take the best policy action otherwise. Note that if there are no legal actions, which is the case at the terminal state, you should choose None as the action. \"\"\" # Pick Action legalActions = self.getLegalActions(state) action = None if legalActions: if util.flipCoin(self.epsilon): return random.choice(legalActions) else: action = self.getPolicy(state) return action def update(self, state, action, nextState, reward): \"\"\" The parent class calls this to observe a state = action =\u003e nextState and reward transition. You should do your Q-Value update here NOTE: You should never call this function, it will be called on your behalf \"\"\" state_action_q_value = self.getQValue(state, action) self.q_values[(state, action)] = state_action_q_value + self.alpha * (reward + self.discount * self.getValue(nextState) - state_action_q_value) def getPolicy(self, state): return self.computeActionFromQValues(sta、te) def getValue(self, state): return self.computeValueFromQValues(state) DQN Q-Learning 依赖于 Q-Table，其存在的问题是当 Q-Table 中的状态非常多，或状态的维度非常多的时候，内存可能无法存储所有的状态，此时我们可以利用神经网络来拟合整个 Q-Table，即使用 Deep Q-Learning Network。DQN 主要用来解决拥有近乎无限的 State，但 Action 有限的问题，它将当前 State 作为输入，输出各个 Action 的 Q-Value。\n启发式搜索和强化学习的对比 ","wordCount":"1875","inLanguage":"en","datePublished":"2020-12-10T20:07:52+08:00","dateModified":"2020-12-10T20:07:52+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://photography.prov1dence.top/posts/reinforcement-learning/heuristic-search-and-reinforcement-learning/"},"publisher":{"@type":"Organization","name":"ChrisChen - 尾張","logo":{"@type":"ImageObject","url":"https://photography.prov1dence.top/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://photography.prov1dence.top/ accesskey=h title="尾張 (Alt + H)">尾張</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://photography.prov1dence.top/archives/ title=Posts><span>Posts</span></a></li><li><a href=https://photography.prov1dence.top/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">启发式搜索和强化学习</h1><div class=post-meta><span title='2020-12-10 20:07:52 +0800 +0800'>December 10, 2020</span>&nbsp;·&nbsp;9 min</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%90%af%e5%8f%91%e5%bc%8f%e6%90%9c%e7%b4%a2%e5%92%8c%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0 aria-label=启发式搜索和强化学习>启发式搜索和强化学习</a><ul><li><a href=#1-%e7%9b%b2%e7%9b%ae%e6%90%9c%e7%b4%a2 aria-label="1 盲目搜索">1 盲目搜索</a><ul><li><a href=#dfs aria-label=DFS>DFS</a></li><li><a href=#bfs aria-label=BFS>BFS</a></li><li><a href=#iterative-deepening-search aria-label="Iterative Deepening Search">Iterative Deepening Search</a></li><li><a href=#uniform-cost-search aria-label="Uniform Cost Search">Uniform Cost Search</a></li></ul></li><li><a href=#2-%e5%90%af%e5%8f%91%e5%bc%8f%e6%90%9c%e7%b4%a2 aria-label="2 启发式搜索">2 启发式搜索</a><ul><li><a href=#%e5%90%af%e5%8f%91%e5%87%bd%e6%95%b0 aria-label=启发函数>启发函数</a></li><li><a href=#a aria-label=A*>A*</a></li></ul></li><li><a href=#3-%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0 aria-label="3 强化学习">3 强化学习</a><ul><li><a href=#%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0 aria-label=强化学习>强化学习</a></li><li><a href=#%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8b aria-label=马尔科夫决策过程>马尔科夫决策过程</a></li><li><a href=#value-iteration aria-label="Value Iteration">Value Iteration</a></li><li><a href=#q-learning aria-label=Q-Learning>Q-Learning</a></li><li><a href=#dqn aria-label=DQN>DQN</a></li><li><a href=#%e5%90%af%e5%8f%91%e5%bc%8f%e6%90%9c%e7%b4%a2%e5%92%8c%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%af%b9%e6%af%94 aria-label=启发式搜索和强化学习的对比>启发式搜索和强化学习的对比</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h1 id=启发式搜索和强化学习>启发式搜索和强化学习<a hidden class=anchor aria-hidden=true href=#启发式搜索和强化学习>#</a></h1><p><a href=https://inst.eecs.berkeley.edu/~cs188/fa18/projects.html>The Pac-Man Projects</a> 是 UC Berkeley CS 188 的课程项目，本文以该项目为例介绍启发式搜索和强化学习。</p><h2 id=1-盲目搜索>1 盲目搜索<a hidden class=anchor aria-hidden=true href=#1-盲目搜索>#</a></h2><p><strong>盲目搜索</strong>（Blind Search）指不利用任何额外信息（输入数据，或辅助函数），只依赖于算法本身的搜索，例如 BFS，DFS，Dijkstra 等；</p><h3 id=dfs>DFS<a hidden class=anchor aria-hidden=true href=#dfs>#</a></h3><p><code>The Pac-Man Projects </code>已经实现了吃豆人游戏的后台逻辑和图形渲染框架，我们只需要在 <code>search.py</code> 文件中实现具体的搜索算法，并根据搜索算法生成寻路路径，即可让吃豆人移动，先来实现一个简单的 DFS：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>DepthFirstSearch</span>(problem):
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> util <span style=color:#f92672>import</span> Stack
</span></span><span style=display:flex><span>    open_list <span style=color:#f92672>=</span> Stack()
</span></span><span style=display:flex><span>    visited <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    open_list<span style=color:#f92672>.</span>push((problem<span style=color:#f92672>.</span>getStartState(), []))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#f92672>not</span> open_list<span style=color:#f92672>.</span>isEmpty():
</span></span><span style=display:flex><span>        current_node, path <span style=color:#f92672>=</span> open_list<span style=color:#f92672>.</span>pop()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> problem<span style=color:#f92672>.</span>isGoalState(current_node):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> path
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> current_node <span style=color:#f92672>in</span> visited:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>        visited<span style=color:#f92672>.</span>append(current_node)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> next_node, action, cost <span style=color:#f92672>in</span> problem<span style=color:#f92672>.</span>getSuccessors(current_node):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> next_node <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> visited:
</span></span><span style=display:flex><span>                open_list<span style=color:#f92672>.</span>push((next_node, path <span style=color:#f92672>+</span> [action]))
</span></span><span style=display:flex><span>dfs <span style=color:#f92672>=</span> DepthFirstSearch
</span></span></code></pre></div><p>在吃豆人游戏的框架下，为寻路函数传入的 <code>problem</code> 参数可以理解为一个 <code>class SearchProblem</code> 类型的抽象基类，实际的问题有 <code>PositionSearchProblem</code>（找到单个终点），<code>FoodSearchProblem</code>（找到所有食物），<code>CapsuleSearchProblem</code>（找到增益药丸和所有食物）等，这些子类都需要实现以下函数：</p><ul><li><code>getStartState()</code>：获取起始状态；</li><li><code>isGoalState(state)</code>：判断 <code>state</code> 节点是否是目标节点；</li><li><code>getSuccessors(statu)</code>：获取 <code>state</code> 节点的所有后续节点；</li><li><code>getCostOfActions(actions)</code>：<code>actions</code> 是一个由上下左右方向组成的一个动作列表，函数返回这个列表的总花费（cost）；</li></ul><p>运行一下看看 DFS 的效果：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>$ python pacman.py -l smallEmpty -z 0.8 -p SearchAgent -a fn<span style=color:#f92672>=</span>dfs     
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>SearchAgent<span style=color:#f92672>]</span> using <span style=color:#66d9ef>function</span> dfs
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>SearchAgent<span style=color:#f92672>]</span> using problem type PositionSearchProblem
</span></span><span style=display:flex><span>Path found with total cost of <span style=color:#ae81ff>56</span> in 0.002992 seconds
</span></span><span style=display:flex><span>Search nodes expanded: <span style=color:#ae81ff>56</span>
</span></span><span style=display:flex><span>Pacman emerges victorious! Score: <span style=color:#ae81ff>454</span>
</span></span><span style=display:flex><span>Average Score: 454.0
</span></span><span style=display:flex><span>Scores:        454.0
</span></span><span style=display:flex><span>Win Rate:      1/1 <span style=color:#f92672>(</span>1.00<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Record:        Win
</span></span></code></pre></div><p>运行的参数列表中有几个参数：</p><ul><li><code>-l smallEmpty</code>：在名为 smallEmpty 的地图上运行，地图定义在 layouts 目录下；</li><li><code>-z 0.8</code>：客户端表现缩放为 0.8 倍</li><li><code>-p SearchAgent</code>：指定实际的问题，这里的 <code>SearchAgent</code> 是 <code>fn='depthFirstSearch', prob='PositionSearchProblem'</code> 的缩写；</li></ul><p>实际运行效果如下：</p><p><img alt=smallempty-dfs loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/smallempty-dfs.gif></p><p>可以看到吃豆人 agent 绕了很远的路才到达终点，因为 DFS 在<a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>计算复杂性理论</a>中是<strong>不完备</strong>（<a href=https://en.wikipedia.org/wiki/Complete_(complexity)>complete</a>）且<strong>非最优</strong>（<a href=https://en.wikipedia.org/wiki/Program_optimization>optimality</a>）的。</p><h3 id=bfs>BFS<a hidden class=anchor aria-hidden=true href=#bfs>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>BreadthFirstSearch</span>(problem):
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> util <span style=color:#f92672>import</span> Queue
</span></span><span style=display:flex><span>    open_list <span style=color:#f92672>=</span> Queue()
</span></span><span style=display:flex><span>    visited <span style=color:#f92672>=</span> set()
</span></span><span style=display:flex><span>    open_list<span style=color:#f92672>.</span>push((problem<span style=color:#f92672>.</span>getStartState(), []))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#f92672>not</span> open_list<span style=color:#f92672>.</span>isEmpty():
</span></span><span style=display:flex><span>        current_node, path <span style=color:#f92672>=</span> open_list<span style=color:#f92672>.</span>pop()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> problem<span style=color:#f92672>.</span>isGoalState(current_node):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> path
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> current_node <span style=color:#f92672>in</span> visited:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>        visited<span style=color:#f92672>.</span>add(current_node)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> next_node, action, cost <span style=color:#f92672>in</span> problem<span style=color:#f92672>.</span>getSuccessors(current_node):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> next_node <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> visited:
</span></span><span style=display:flex><span>                open_list<span style=color:#f92672>.</span>push((next_node, path <span style=color:#f92672>+</span> [action]))
</span></span><span style=display:flex><span>bfs <span style=color:#f92672>=</span> BreadthFirstSearch
</span></span></code></pre></div><p>BFS 的运行效果如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>$ python pacman.py -l smallEmpty -z 0.8 -p SearchAgent -a fn<span style=color:#f92672>=</span>bfs
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>SearchAgent<span style=color:#f92672>]</span> using <span style=color:#66d9ef>function</span> bfs
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>SearchAgent<span style=color:#f92672>]</span> using problem type PositionSearchProblem
</span></span><span style=display:flex><span>Path found with total cost of <span style=color:#ae81ff>14</span> in 0.001995 seconds
</span></span><span style=display:flex><span>Search nodes expanded: <span style=color:#ae81ff>63</span>
</span></span><span style=display:flex><span>Pacman emerges victorious! Score: <span style=color:#ae81ff>496</span>
</span></span><span style=display:flex><span>Average Score: 496.0
</span></span><span style=display:flex><span>Scores:        496.0
</span></span><span style=display:flex><span>Win Rate:      1/1 <span style=color:#f92672>(</span>1.00<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Record:        Win
</span></span></code></pre></div><p><img alt=smallempty-bfs loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/smallempty-bfs.gif></p><p>可以看到使用 BFS 的 agent 通过最短路径到达了终点，因为 BFS 是<strong>完备</strong>且<strong>最优</strong>的。</p><h3 id=iterative-deepening-search>Iterative Deepening Search<a hidden class=anchor aria-hidden=true href=#iterative-deepening-search>#</a></h3><p>IDS 的思路是重复进行限制层数的 DFS 来找到最优解，它综合了 DFS 的优点（空间复杂度）和 BFS 的优点（完备且最优），但是在时间复杂度上表现比较差（可以参考输出结果中的 Search nodes expanded）：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>IterativeDeepeningSearch</span>(problem):
</span></span><span style=display:flex><span>    <span style=color:#f92672>import</span> sys
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> util <span style=color:#f92672>import</span> Stack
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>depthLimitSearch</span>(problem, depth):
</span></span><span style=display:flex><span>        visited <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        open_list <span style=color:#f92672>=</span> Stack()
</span></span><span style=display:flex><span>        open_list<span style=color:#f92672>.</span>push((problem<span style=color:#f92672>.</span>getStartState(), [], visited))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> <span style=color:#f92672>not</span> open_list<span style=color:#f92672>.</span>isEmpty():
</span></span><span style=display:flex><span>            current_node, path, visited <span style=color:#f92672>=</span> open_list<span style=color:#f92672>.</span>pop()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> problem<span style=color:#f92672>.</span>isGoalState(current_node):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>return</span> path
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> len(path) <span style=color:#f92672>==</span> depth <span style=color:#f92672>or</span> depth <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> current_node <span style=color:#f92672>in</span> visited:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>            actions <span style=color:#f92672>=</span> problem<span style=color:#f92672>.</span>getSuccessors(current_node)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> next_node, action, cost <span style=color:#f92672>in</span> actions:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> next_node <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> visited:
</span></span><span style=display:flex><span>                    open_list<span style=color:#f92672>.</span>push((next_node, path <span style=color:#f92672>+</span> [action], visited<span style=color:#f92672>+</span>[current_node]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> depth <span style=color:#f92672>in</span> range(sys<span style=color:#f92672>.</span>maxsize<span style=color:#f92672>**</span><span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>        path <span style=color:#f92672>=</span> depthLimitSearch(problem, depth)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> path:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> path
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>ids <span style=color:#f92672>=</span> IterativeDeepeningSearch
</span></span></code></pre></div><p>这个算法对于只有小面积可搜索空间的地图效果比较好：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>$ python pacman.py -l smallMaze -z 0.8 -p SearchAgent -a fn<span style=color:#f92672>=</span>ids
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>SearchAgent<span style=color:#f92672>]</span> using <span style=color:#66d9ef>function</span> ids
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>SearchAgent<span style=color:#f92672>]</span> using problem type PositionSearchProblem
</span></span><span style=display:flex><span>Path found with total cost of <span style=color:#ae81ff>19</span> in 0.008976 seconds
</span></span><span style=display:flex><span>Search nodes expanded: <span style=color:#ae81ff>923</span>
</span></span><span style=display:flex><span>Pacman emerges victorious! Score: <span style=color:#ae81ff>491</span>
</span></span><span style=display:flex><span>Average Score: 491.0
</span></span><span style=display:flex><span>Scores:        491.0
</span></span><span style=display:flex><span>Win Rate:      1/1 <span style=color:#f92672>(</span>1.00<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Record:        Win
</span></span></code></pre></div><p><img alt=smallmaze-ids loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/smallmaze-ids.gif></p><p>但是对于拥有大面积可搜索空间的地图，搜索时间会非常长：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>$ python pacman.py -l smallEmpty -z 0.8 -p SearchAgent -a fn<span style=color:#f92672>=</span>ids
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>SearchAgent<span style=color:#f92672>]</span> using <span style=color:#66d9ef>function</span> ids
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>SearchAgent<span style=color:#f92672>]</span> using problem type PositionSearchProblem
</span></span><span style=display:flex><span>Path found with total cost of <span style=color:#ae81ff>14</span> in 0.710854 seconds
</span></span><span style=display:flex><span>Search nodes expanded: <span style=color:#ae81ff>94552</span>
</span></span><span style=display:flex><span>Pacman emerges victorious! Score: <span style=color:#ae81ff>496</span>
</span></span><span style=display:flex><span>Average Score: 496.0
</span></span><span style=display:flex><span>Scores:        496.0
</span></span><span style=display:flex><span>Win Rate:      1/1 <span style=color:#f92672>(</span>1.00<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Record:        Win
</span></span></code></pre></div><p><img alt=smallempty-ids loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/smallempty-ids.gif></p><h3 id=uniform-cost-search>Uniform Cost Search<a hidden class=anchor aria-hidden=true href=#uniform-cost-search>#</a></h3><p>UCS 和 Dijkstra 类似，用一个小根堆保存当前节点到起始节点的距离，依次展开路径花费最小的节点，直到找到终点为止，而一般来说 Dijkstra 没有一个固定的终点：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>UniformCostSearch</span>(problem):
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> util <span style=color:#f92672>import</span> PriorityQueue
</span></span><span style=display:flex><span>    frontier <span style=color:#f92672>=</span> PriorityQueue()
</span></span><span style=display:flex><span>    visited <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    frontier<span style=color:#f92672>.</span>push((problem<span style=color:#f92672>.</span>getStartState(), [], <span style=color:#ae81ff>0</span>), <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#f92672>not</span> frontier<span style=color:#f92672>.</span>isEmpty():
</span></span><span style=display:flex><span>        current_node, path, current_cost <span style=color:#f92672>=</span> frontier<span style=color:#f92672>.</span>pop()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> problem<span style=color:#f92672>.</span>isGoalState(current_node):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> path
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> current_node <span style=color:#f92672>in</span> visited:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>        visited<span style=color:#f92672>.</span>append(current_node)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> next_node, action, cost <span style=color:#f92672>in</span> problem<span style=color:#f92672>.</span>getSuccessors(current_node):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> next_node <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> visited:
</span></span><span style=display:flex><span>                frontier<span style=color:#f92672>.</span>push((next_node, path <span style=color:#f92672>+</span> [action], current_cost <span style=color:#f92672>+</span> cost), current_cost <span style=color:#f92672>+</span> cost)
</span></span><span style=display:flex><span>ucs <span style=color:#f92672>=</span> UniformCostSearch
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>$ python pacman.py -l smallEmpty -z 0.8 -p SearchAgent -a fn<span style=color:#f92672>=</span>ucs
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>SearchAgent<span style=color:#f92672>]</span> using <span style=color:#66d9ef>function</span> ucs
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>SearchAgent<span style=color:#f92672>]</span> using problem type PositionSearchProblem
</span></span><span style=display:flex><span>Path found with total cost of <span style=color:#ae81ff>14</span> in 0.002992 seconds
</span></span><span style=display:flex><span>Search nodes expanded: <span style=color:#ae81ff>63</span>
</span></span><span style=display:flex><span>Pacman emerges victorious! Score: <span style=color:#ae81ff>496</span>
</span></span><span style=display:flex><span>Average Score: 496.0
</span></span><span style=display:flex><span>Scores:        496.0
</span></span><span style=display:flex><span>Win Rate:      1/1 <span style=color:#f92672>(</span>1.00<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Record:        Win
</span></span></code></pre></div><p><img alt=smallempty-ucs loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/smallempty-ucs.gif></p><h2 id=2-启发式搜索>2 启发式搜索<a hidden class=anchor aria-hidden=true href=#2-启发式搜索>#</a></h2><p>传统的盲目搜索算法因为受制于完备性、最优性、时间、空间复杂度等因素，在实际的应用中很少被使用；而在路径规划，最优化算法和人工智能领域，使用<strong>启发式搜索</strong>（Heuristic Search）能够更好地在准确性和计算速度之间取得平衡。</p><p><strong>启发式搜索</strong> <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>Heuristic Search</a> 又叫做有信息搜索 Informed Search，启发式搜索不同于盲目搜索的地方有两点：一是启发式搜索依赖于启发函数，<strong>启发函数</strong> Heuristic Function 是用于<strong>估计</strong>当前节点到目标节点距离的一类函数；二是它需要利用<strong>输入数据</strong>并将其作为启发函数的参数，以衡量当前位置到目标位置的距离关系。</p><p>启发式搜索通过衡量当前位置到目标位置的距离关系，使得搜索过程的移动方向优先朝向目标位置更近的方向前进，以提高搜索效率。</p><h3 id=启发函数>启发函数<a hidden class=anchor aria-hidden=true href=#启发函数>#</a></h3><p>启发函数 h(n) 用于给出从特定节点到目标节点的距离的<strong>估计值</strong>（而不是真实值）；许多寻路问题都是 NP 完备（<a href=https://en.wikipedia.org/wiki/NP-completeness>NP-completeness</a>）的，因此在最坏情况下它们的算法时间复杂度都是指数级的；找到一个好的启发函数可以更高效地得到一个更优的解；启发函数算法的优劣直接决定了启发式搜索的效率。</p><p>最简单的启发函数有：</p><ul><li>null heuristic：估计值始终等于 0，相当于退化成了 UCS（只计算当前节点到起始节点的距离）；</li><li>曼哈顿距离：两点在南北方向上的距离加上在东西方向上的距离，即 <code>abs(a − x) + abs(b − y)</code>；</li><li>欧几里得距离：两点在欧氏空间中的直线距离，即 <code>sqrt((a - x) ^ 2 + (b - y) ^ 2)</code>；</li></ul><h3 id=a>A*<a hidden class=anchor aria-hidden=true href=#a>#</a></h3><p>A* 是一种应用很广泛的启发式搜索算法，其主要思路与 Dijkstra 和 UCS 类似，都是利用一个小根堆，不断地取出堆顶节点并判断其是否是目标节点，不同的是它会为每一个已知节点计算出从起点和终点的距离之和 <code>f(x) = g(x) + h(x)</code>，其中 <code>g(x)</code> 是从起点到当前节点的实际距离，<code>h(x)</code> 是使用启发函数计算得到的从当前节点到目标节点的估计距离：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>AStarSearch</span>(problem, heuristic<span style=color:#f92672>=</span>nullHeuristic):
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> util <span style=color:#f92672>import</span> PriorityQueueWithFunction
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>AStarHeuristic</span>(item):
</span></span><span style=display:flex><span>        state, _, cost <span style=color:#f92672>=</span> item
</span></span><span style=display:flex><span>        h <span style=color:#f92672>=</span> heuristic(state, problem<span style=color:#f92672>=</span>problem)
</span></span><span style=display:flex><span>        g <span style=color:#f92672>=</span> cost
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> g <span style=color:#f92672>+</span> h
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    frontier <span style=color:#f92672>=</span> PriorityQueueWithFunction(AStarHeuristic)
</span></span><span style=display:flex><span>    visited <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    frontier<span style=color:#f92672>.</span>push((problem<span style=color:#f92672>.</span>getStartState(),[], <span style=color:#ae81ff>0</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#f92672>not</span> frontier<span style=color:#f92672>.</span>isEmpty():
</span></span><span style=display:flex><span>        currentNode, path, currentCost <span style=color:#f92672>=</span> frontier<span style=color:#f92672>.</span>pop()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> problem<span style=color:#f92672>.</span>isGoalState(currentNode):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> path
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> currentNode <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> visited:
</span></span><span style=display:flex><span>            visited<span style=color:#f92672>.</span>append(currentNode)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> nextNode, action, cost <span style=color:#f92672>in</span> problem<span style=color:#f92672>.</span>getSuccessors(currentNode):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> nextNode <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> visited:
</span></span><span style=display:flex><span>                    frontier<span style=color:#f92672>.</span>push((nextNode, path <span style=color:#f92672>+</span> [action], currentCost <span style=color:#f92672>+</span> cost))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>astar <span style=color:#f92672>=</span> AStarSearch
</span></span></code></pre></div><p>对于多节点的搜索问题，需要综合考虑所有目标节点对于当前节点的影响；我们可以利用贪心的思想，让吃豆人优先靠近距离较近的豆子，也就是使得距离当前节点更近的目标节点的启发函数值更小，这样距离吃豆人更近的豆子就会更有可能具有更小的 <code>f(x)</code> 值；在为启发函数传入的参数中， <code>state</code> 是一个包含当前位置 <code>position</code> 和所有目标点信息结构 <code>grid</code> 的二元组，可以使用 <code>grid.asList()</code> 将所有目标点转换为一个数组：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>FoodHeuristic</span>(state, problem):
</span></span><span style=display:flex><span>    position, food_grid <span style=color:#f92672>=</span> state
</span></span><span style=display:flex><span>    food_gridList <span style=color:#f92672>=</span> food_grid <span style=color:#66d9ef>if</span> isinstance(food_grid, list) <span style=color:#66d9ef>else</span> food_grid<span style=color:#f92672>.</span>asList()
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> util <span style=color:#f92672>import</span> manhattanDistance
</span></span><span style=display:flex><span>    minx, miny <span style=color:#f92672>=</span> position
</span></span><span style=display:flex><span>    maxx, maxy <span style=color:#f92672>=</span> position
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> food <span style=color:#f92672>in</span> food_gridList:
</span></span><span style=display:flex><span>        foodx, foody <span style=color:#f92672>=</span> food
</span></span><span style=display:flex><span>        minx <span style=color:#f92672>=</span> min(foodx,minx)
</span></span><span style=display:flex><span>        maxx <span style=color:#f92672>=</span> max(foodx,maxx)
</span></span><span style=display:flex><span>        miny <span style=color:#f92672>=</span> min(foody,miny)
</span></span><span style=display:flex><span>        maxy <span style=color:#f92672>=</span> max(foody,maxy)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> abs(minx<span style=color:#f92672>-</span>maxx) <span style=color:#f92672>+</span> abs(miny<span style=color:#f92672>-</span>maxy)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>$ python pacman.py -l tinySearch -p SearchAgent -a fn<span style=color:#f92672>=</span>astar,prob<span style=color:#f92672>=</span>FoodSearchProblem,heuristic<span style=color:#f92672>=</span>FoodHeuristic
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>SearchAgent<span style=color:#f92672>]</span> using <span style=color:#66d9ef>function</span> astar and heuristic foodHeuristic
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>SearchAgent<span style=color:#f92672>]</span> using problem type FoodSearchProblem
</span></span><span style=display:flex><span>Path found with total cost of <span style=color:#ae81ff>27</span> in 0.294214 seconds
</span></span><span style=display:flex><span>Search nodes expanded: <span style=color:#ae81ff>1544</span>
</span></span><span style=display:flex><span>Pacman emerges victorious! Score: <span style=color:#ae81ff>573</span>
</span></span><span style=display:flex><span>Average Score: 573.0
</span></span><span style=display:flex><span>Scores:        573.0
</span></span><span style=display:flex><span>Win Rate:      1/1 <span style=color:#f92672>(</span>1.00<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Record:        Win
</span></span></code></pre></div><p><img alt=tinySearch-astar loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/tinySearch-astar.gif></p><p>换个地图看看效果：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>$ python pacman.py -l mediumDottedMaze -p SearchAgent -a fn<span style=color:#f92672>=</span>astar,prob<span style=color:#f92672>=</span>FoodSearchProblem,heuristic<span style=color:#f92672>=</span>FoodHeuristic
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>SearchAgent<span style=color:#f92672>]</span> using <span style=color:#66d9ef>function</span> astar and heuristic foodHeuristic
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>SearchAgent<span style=color:#f92672>]</span> using problem type FoodSearchProblem
</span></span><span style=display:flex><span>Path found with total cost of <span style=color:#ae81ff>74</span> in 0.091756 seconds
</span></span><span style=display:flex><span>Search nodes expanded: <span style=color:#ae81ff>389</span>
</span></span><span style=display:flex><span>Pacman emerges victorious! Score: <span style=color:#ae81ff>646</span>
</span></span><span style=display:flex><span>Average Score: 646.0
</span></span><span style=display:flex><span>Scores:        646.0
</span></span><span style=display:flex><span>Win Rate:      1/1 <span style=color:#f92672>(</span>1.00<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Record:        Win
</span></span></code></pre></div><p><img alt=mediumdottedmaze-astar loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/mediumdottedmaze-astar.gif></p><p>相比之下，如果使用 <code>nullHeuristic</code>（退化为 UCS）的话搜索花费的时间则会长很多：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>$ python pacman.py -l tinySearch -p SearchAgent -a fn<span style=color:#f92672>=</span>ucs,prob<span style=color:#f92672>=</span>FoodSearchProblem
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>SearchAgent<span style=color:#f92672>]</span> using <span style=color:#66d9ef>function</span> ucs
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>SearchAgent<span style=color:#f92672>]</span> using problem type FoodSearchProblem
</span></span><span style=display:flex><span>Path found with total cost of <span style=color:#ae81ff>27</span> in 2.880744 seconds
</span></span><span style=display:flex><span>Search nodes expanded: <span style=color:#ae81ff>5057</span>
</span></span><span style=display:flex><span>Pacman emerges victorious! Score: <span style=color:#ae81ff>573</span>
</span></span><span style=display:flex><span>Average Score: 573.0
</span></span><span style=display:flex><span>Scores:        573.0
</span></span><span style=display:flex><span>Win Rate:      1/1 <span style=color:#f92672>(</span>1.00<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Record:        Win
</span></span></code></pre></div><p><img alt=tinySearch-astar-null-heuristic loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/tinySearch-astar-null-heuristic.gif></p><h2 id=3-强化学习>3 强化学习<a hidden class=anchor aria-hidden=true href=#3-强化学习>#</a></h2><h3 id=强化学习>强化学习<a hidden class=anchor aria-hidden=true href=#强化学习>#</a></h3><p>强化学习是指通过与环境进行交互和反馈来学习一种策略的过程，在这个过程中，一个强化学习的实体 <em>agent</em> 通过与环境 <em>Environment</em> 进行交互并采取一系列行为 <em>Action</em> 来获得一定的收益 <em>Reward</em>，从而更新采取相应行为的权重。</p><p><img alt=reinforcement-learning-process loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/reinforcement-learning-process.png></p><p>强化学习的目标是通过学习得到某个策略 <em>Policy</em>，使得 <em>agent</em> 从 <em>environment</em> 中获得的<strong>长期收益</strong>最大化，因此在一般的问题中，在没有达到最终的目的前，<em>reward</em> 通常都是负数（随时间的增加而减少），而仅在达到最终的目的时获得较大的正反馈，这样的学习任务通常称为 episodic task（例如吃豆人游戏中的单节点搜索问题）；在另一类问题中，可能需要完成多个目标才能到达最终状态，其 <em>reward</em> 离散地分布在一个连续的空间中，这一类任务称为 continuing task（例如吃豆人游戏中的多节点搜索问题），对于 continuing task，我们可以定义其 reward 为：</p><p><img alt=discounted-reward loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/discounted-reward.svg></p><p>其中 <em>γ</em> 是衰减率（discount factor），衰减率可以使得我们更加偏好近期收益；引入衰减系数的理由有很多，例如避免陷入无限循环，降低远期利益的不确定性，最大化近期利益，利用近期利益产生新的利益因而其更有价值等等。</p><p>而强化学习的结果是就是 <em>Gt</em>，通过 <em>argmax</em> 取得的值能够给出在每个状态下我们应该采取的行动，我们可以把这个策略记做 <em>π(a|s)</em>，它表示在状态 s 下采取行动 a 的概率。</p><h3 id=马尔科夫决策过程>马尔科夫决策过程<a hidden class=anchor aria-hidden=true href=#马尔科夫决策过程>#</a></h3><p><strong>马尔科夫决策过程</strong>（<a href=https://en.wikipedia.org/wiki/Markov_decision_process>Markov Decision Process</a>, MDP）是指在每个状态下，agent 对于行动 <em>a</em> 的选取只依赖于当前的状态，与任何之前的行为都没有关系；几乎所有的强化学习问题都可以使用 MDP 解决，一个标准的马尔可夫决策过程由一个四元组组成：</p><ul><li><em>S</em>：<em>State</em>，<strong>状态</strong>空间的集合，<em>S0</em> 表示初始状态；</li><li><em>A</em>：<em>Action</em>，<strong>行为</strong>空间的集合，包含每个状态可以进行的动作；</li><li><em>r(s&rsquo; | s, a)</em>：<em>Reward</em>，在 s 状态下，进行 a 操作并转移到 s‘ 状态下的<strong>奖励</strong>；</li><li><em>P(s&rsquo; | s, a)</em>：<em>Probability</em>，在 s 状态下，进行 a 操作并转移到 s‘ 状态下的<strong>概率</strong>；</li></ul><p>求解 MDP 问题的常见方法有 Value iteration，Policy iteration，Q-Learning，Deep Q-Learning Network 等等。</p><h3 id=value-iteration>Value Iteration<a hidden class=anchor aria-hidden=true href=#value-iteration>#</a></h3><p>Value Iteration 是一种基于模型的（model-based）算法，使用 Value Iteration 来解决 MDP 问题的前提是我们知道关于模型的所有信息，即 MDP 四元组的所有内容。</p><p>假设现在有一个 3*4 叫做 GridWorld 的地图如图所示，以左下角格子为 (0, 0) 原点，其中 (1, 1) 为不可通过的墙，(2, 3) 为奖励为 +1 的终点，(1, 3) 为 -1 的终点；我们定义每一个位置的价值为 <em>V(state)</em>，即对于 <em>state(x, y)</em>，<em>V(state)</em> 表示其能获取的最大价值；每一个位置初始化时其 <em>value</em> 均为 0：</p><p><img alt=value-iteration-0 loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/value-iteration-0.png></p><p>在迭代过程中，使用贝尔曼方程（<a href=https://en.wikipedia.org/wiki/Bellman_equation>Bellman Equation</a>）更新<strong>所有位置</strong>的 <em>value</em>，它描述了最佳策略必须满足的条件，前半部分 <em>r(s, a, s&rsquo;)</em> 代表采取了 <em>a</em> 行为之后得到的 reward，后半部分；我们需要在每轮迭代中计算每个状态的价值即 <em>V(s)</em>，直到两次迭代结果的差值小于给定的阈值才能认为其收敛了，这里的 <em>V(s)</em> 也叫做 q-value：</p><p><img alt=bellman-equation loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/bellman-equation.png></p><p>经过前三次迭代分别得到：</p><p><img alt=value-iteration-1 loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/value-iteration-1.png></p><p><img alt=value-iteration-2 loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/value-iteration-2.png></p><p><img alt=value-iteration-3 loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/value-iteration-3.png></p><p>收敛速度是指数级，并且随着迭代的不断进行，终将得到最优的 <em>V(s)</em>；或者说当迭代次数趋近于无穷大的时候，将得到 <em>V(s)</em> 的最优解；经过 100 次迭代后将得到：</p><p><img alt=value-iteration-100 loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/value-iteration-100.png></p><p>取 argmax 即可得到最优的策略（即上图中的小箭头）；也可以看到采取每一种行动对应的 Probaility：</p><p><img alt=value-iteration-100-argmax loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/value-iteration-100-argmax.png></p><p>进行 Value Iteration 的流程主要对应 <code>runValueIteration</code> 和 <code>computeQValueFromValues</code> 两个函数，迭代结束后选择策略则对应 <code>computeActionFromValues</code> 函数：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># valueIterationAgents.py</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ValueIterationAgent</span>(ValueEstimationAgent):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        A ValueIterationAgent takes a Markov decision process
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        (see mdp.py) on initialization and runs value iteration
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        for a given number of iterations using the supplied
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        discount factor.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, mdp, discount <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.9</span>, iterations <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          Some useful mdp methods you will use:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>              mdp.getStates()
</span></span></span><span style=display:flex><span><span style=color:#e6db74>              mdp.getPossibleActions(state)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>              mdp.getTransitionStatesAndProbs(state, action)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>              mdp.getReward(state, action, nextState)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>              mdp.isTerminal(state)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>mdp <span style=color:#f92672>=</span> mdp
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>discount <span style=color:#f92672>=</span> discount
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>iterations <span style=color:#f92672>=</span> iterations
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>values <span style=color:#f92672>=</span> util<span style=color:#f92672>.</span>Counter() <span style=color:#75715e># A Counter is a dict with default 0</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>runValueIteration()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>runValueIteration</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, self<span style=color:#f92672>.</span>iterations):
</span></span><span style=display:flex><span>            next_values <span style=color:#f92672>=</span> util<span style=color:#f92672>.</span>Counter()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> state <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>mdp<span style=color:#f92672>.</span>getStates():
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>mdp<span style=color:#f92672>.</span>isTerminal(state):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                q_values <span style=color:#f92672>=</span> util<span style=color:#f92672>.</span>Counter()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> action <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>mdp<span style=color:#f92672>.</span>getPossibleActions(state):
</span></span><span style=display:flex><span>                    q_values[action] <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>computeQValueFromValues(state, action)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                key_max_value <span style=color:#f92672>=</span> q_values<span style=color:#f92672>.</span>argMax()
</span></span><span style=display:flex><span>                next_values[state] <span style=color:#f92672>=</span> q_values[key_max_value]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>values <span style=color:#f92672>=</span> next_values
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>getValue</span>(self, state):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>values[state]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>computeQValueFromValues</span>(self, state, action):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          Compute the Q-value of action in state from the
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          value function stored in self.values.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        next_states_probs <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>mdp<span style=color:#f92672>.</span>getTransitionStatesAndProbs(state, action)
</span></span><span style=display:flex><span>        q_value <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> (next_state, next_state_prob) <span style=color:#f92672>in</span> next_states_probs:
</span></span><span style=display:flex><span>            q_value <span style=color:#f92672>+=</span> next_state_prob <span style=color:#f92672>*</span> (self<span style=color:#f92672>.</span>mdp<span style=color:#f92672>.</span>getReward(state, action, next_state) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>discount <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>values[next_state])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> q_value
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>computeActionFromValues</span>(self, state):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          The policy is the best action in the given state
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          according to the values currently stored in self.values.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          You may break ties any way you see fit.  Note that if
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          there are no legal actions, which is the case at the
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          terminal state, you should return None.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>mdp<span style=color:#f92672>.</span>isTerminal(state):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        actions <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>mdp<span style=color:#f92672>.</span>getPossibleActions(state)
</span></span><span style=display:flex><span>        values <span style=color:#f92672>=</span> util<span style=color:#f92672>.</span>Counter()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> action <span style=color:#f92672>in</span> actions:
</span></span><span style=display:flex><span>            values[action] <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>computeQValueFromValues(state, action)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        policy <span style=color:#f92672>=</span> values<span style=color:#f92672>.</span>argMax()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> policy
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>getPolicy</span>(self, state):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>computeActionFromValues(state)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>getAction</span>(self, state):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;Returns the policy at the state (no exploration).&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>computeActionFromValues(state)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>getQValue</span>(self, state, action):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>computeQValueFromValues(state, action)
</span></span></code></pre></div><h3 id=q-learning>Q-Learning<a hidden class=anchor aria-hidden=true href=#q-learning>#</a></h3><p><a href=https://en.wikipedia.org/wiki/Q-learning>Q-Learning</a> 的思路与 Value Iteration 有一些类似，但它是一种模型无关的（model-free）算法，使用 Q-Learning 的时候我们的 agent 无需事先知道当前环境中的 <em>State</em>，<em>Action</em> 等 MDP 四元组内容，</p><p>在使用 Value Ietration 的时候，我们需要在每一个 episode 对所有的 <em>State</em> 和 <em>Action</em> 进行更新，但在实际问题中 <em>State</em> 的数量可能非常多以致于我们不可能遍历完所有的状态，这时候我们可以借助 Q-Learning，在对于环境未知的前提下，不断地与环境进行交互和探索，计算出有限的环境样本中 Q-Value，并维护一个 Q-Table：</p><table><thead><tr><th><em>S</em></th><th><em>r(s&rsquo; | s, action 1)</em></th><th><em>r(s&rsquo; | s, action 2)</em></th><th>&mldr;</th></tr></thead><tbody><tr><td><em>S1</em> (0, 0)</td><td>3</td><td>-1</td><td></td></tr><tr><td><em>S2</em> (0, 1)</td><td>-2</td><td>4</td><td></td></tr><tr><td>&mldr;</td><td></td><td></td><td></td></tr></tbody></table><p>在刚开始时，agent 对于环境一无所知，因此 Q-Table 应该被初始化为一个零矩阵；当我们处于某个状态 <em>s</em> （例如表里的 <em>S1</em>）时，根据 Q-Table 中当前的最优值和一定的策略（<a href=https://en.wikipedia.org/wiki/Multi-armed_bandit>Multi-armed bandit</a> 问题，利用 ε-greedy，UCB 等解决）选择对应的动作 <em>a</em> （假设选择了表里的 <em>action 1</em>，对应 <em>r = 3</em>）进行探索，并根据获得的即时奖励 <em>r</em> 来更新奖励，这里的 <em>r</em> 只是即时获得的奖励（<em>r = 3</em>），因为还要考虑所转移到的状态 <em>s&rsquo;</em> （表里的 <em>S2</em>）在未来可能会获取到的最大奖励（<em>r&rsquo; = 4</em>）；</p><p><img alt=q-learning-paper loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/q-learning-paper.png></p><p>真正的奖励 <em>Q(St, At)</em> 由公式中的两部分组成，前半部分 <em>r</em> 是通过动作 <em>a</em> 即时获得的奖励（<em>r = 3</em>），后半部分 <em>γ * max(a&rsquo;)Q(s&rsquo;, a&rsquo;)</em> 是对未来行为的最大期望奖励（<em>r&rsquo; = 4</em>），且后半部分往往是不确定的，因此需要乘以衰减率 <em>γ</em>：</p><p><img alt=q-function loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/q-function.png></p><p>在通过计算得到当前行为所能获得的预期奖励后，将其减去表中对当前环境的估计奖励 <em>Q(s,a)</em>（<em>r = 3</em>），再乘以学习率，就能用来更新 Q-Table 中的值了。</p><p>agent 不断地与环境进行探索并发生状态转换，直到到达目标；我们将 agent 的每一轮探索（从任意起始状态出发，经历若干个 <em>action</em>，直到到达目标状态）称为一个 episode；在进行指定 episode 次数的训练之后，取 argmax 得到的策略就是当前的最优解。</p><p>现在利用 epsilon-greedy 作为探索策略，训练 10 个 episode：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>$ python gridworld.py -a q -k <span style=color:#ae81ff>10</span> --noise 0.0 -e 0.9
</span></span></code></pre></div><p><img alt=q-learning-epsilon-greedy-10-episodes loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/q-learning-epsilon-greedy-10-episodes.gif></p><p>得到结果之后，对每一个状态 <em>s</em> 的所有动作 <em>a</em> 取 <em>argmax</em> 即可得到在当前 epsilon 值和 episode 值下的最优解：</p><p><img alt=q-learning-epsilon-greedy-10-episodes loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/q-learning-epsilon-greedy-10-episodes.png></p><p>Q-Learning 的实现大致如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>QLearningAgent</span>(ReinforcementAgent):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, <span style=color:#f92672>**</span>args):
</span></span><span style=display:flex><span>        ReinforcementAgent<span style=color:#f92672>.</span>__init__(self, <span style=color:#f92672>**</span>args)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>q_values <span style=color:#f92672>=</span> defaultdict(<span style=color:#66d9ef>lambda</span>: <span style=color:#ae81ff>0.0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>getQValue</span>(self, state, action):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          Returns Q(state,action)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          Should return 0.0 if we have never seen a state
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          or the Q node value otherwise
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>q_values[(state, action)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>computeValueFromQValues</span>(self, state):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          Returns max_action Q(state,action)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          where the max is over legal actions.  Note that if
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          there are no legal actions, which is the case at the
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          terminal state, you should return a value of 0.0.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        next_actions <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>getLegalActions(state)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> next_actions:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            q_value_actions <span style=color:#f92672>=</span> [(self<span style=color:#f92672>.</span>getQValue(state, action), action) <span style=color:#66d9ef>for</span> action <span style=color:#f92672>in</span> next_actions]
</span></span><span style=display:flex><span>            <span style=color:#75715e># return the max in q_value_actions which is q_value</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> sorted(q_value_actions, key<span style=color:#f92672>=</span><span style=color:#66d9ef>lambda</span> x: x[<span style=color:#ae81ff>0</span>])[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>computeActionFromQValues</span>(self, state):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          Compute the best action to take in a state.  Note that if there
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          are no legal actions, which is the case at the terminal state,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          you should return None.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        next_actions <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>getLegalActions(state)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> next_actions:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            actions <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>            max_q_value <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>getQValue(state, next_actions[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># find actions with max q value</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> action <span style=color:#f92672>in</span> next_actions:
</span></span><span style=display:flex><span>                action_q_value <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>getQValue(state, action)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> max_q_value <span style=color:#f92672>&lt;</span> action_q_value:
</span></span><span style=display:flex><span>                    max_q_value <span style=color:#f92672>=</span> action_q_value
</span></span><span style=display:flex><span>                    actions <span style=color:#f92672>=</span> [action]
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>elif</span> max_q_value <span style=color:#f92672>==</span> action_q_value:
</span></span><span style=display:flex><span>                    actions<span style=color:#f92672>.</span>append(action)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># break ties randomly for better behavior. The random.choice() function will help.</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> random<span style=color:#f92672>.</span>choice(actions)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>getAction</span>(self, state):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          Compute the action to take in the current state.  With
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          probability self.epsilon, we should take a random action and
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          take the best policy action otherwise.  Note that if there are
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          no legal actions, which is the case at the terminal state, you
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          should choose None as the action.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Pick Action</span>
</span></span><span style=display:flex><span>        legalActions <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>getLegalActions(state)
</span></span><span style=display:flex><span>        action <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> legalActions:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> util<span style=color:#f92672>.</span>flipCoin(self<span style=color:#f92672>.</span>epsilon):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>return</span> random<span style=color:#f92672>.</span>choice(legalActions)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                action <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>getPolicy(state)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> action
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update</span>(self, state, action, nextState, reward):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          The parent class calls this to observe a
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          state = action =&gt; nextState and reward transition.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          You should do your Q-Value update here
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          NOTE: You should never call this function,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          it will be called on your behalf
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        state_action_q_value <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>getQValue(state, action)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>q_values[(state, action)] <span style=color:#f92672>=</span> state_action_q_value <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>alpha <span style=color:#f92672>*</span> (reward <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>discount <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>getValue(nextState) <span style=color:#f92672>-</span> state_action_q_value)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>getPolicy</span>(self, state):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>computeActionFromQValues(sta<span style=color:#960050;background-color:#1e0010>、</span>te)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>getValue</span>(self, state):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>computeValueFromQValues(state)
</span></span></code></pre></div><h3 id=dqn>DQN<a hidden class=anchor aria-hidden=true href=#dqn>#</a></h3><p>Q-Learning 依赖于 Q-Table，其存在的问题是当 Q-Table 中的状态非常多，或状态的维度非常多的时候，内存可能无法存储所有的状态，此时我们可以利用神经网络来拟合整个 Q-Table，即使用 <a href=https://www.tensorflow.org/agents/tutorials/0_intro_rl>Deep Q-Learning Network</a>。DQN 主要用来解决拥有近乎无限的 <em>State</em>，但 <em>Action</em> 有限的问题，它将当前 <em>State</em> 作为输入，输出各个 <em>Action</em> 的 Q-Value。</p><p><img alt=deep-q-learning-network loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/deep-q-learning-network.png></p><h3 id=启发式搜索和强化学习的对比>启发式搜索和强化学习的对比<a hidden class=anchor aria-hidden=true href=#启发式搜索和强化学习的对比>#</a></h3><p><img alt=pacman-contest loading=lazy src=https://raw.githubusercontent.com/chr1sc2y/warehouse-deprecated/refs/heads/main/resources/reinforcement-learning/pacman-contest.gif></p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://photography.prov1dence.top/posts/cpp/smart-pointer/smart-pointer/><span class=title>« Prev</span><br><span>C++ 智能指针的简单实现</span>
</a><a class=next href=https://photography.prov1dence.top/posts/cpp/closure-and-anonymous-function/><span class=title>Next »</span><br><span>C++ 闭包和匿名函数</span></a></nav></footer></article></main><footer class=footer></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>